{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:12:32.836833Z",
     "iopub.status.busy": "2022-05-01T11:12:32.836233Z",
     "iopub.status.idle": "2022-05-01T11:12:32.840981Z",
     "shell.execute_reply": "2022-05-01T11:12:32.840179Z",
     "shell.execute_reply.started": "2022-05-01T11:12:32.836791Z"
    },
    "id": "kJbQJTVqbUGX"
   },
   "outputs": [],
   "source": [
    "token = \"Y9czvVBhfFYY1KG7GzBhtwOm2whsjGcHQ4bBHoPn32_EGGrGL0GvaEKV5Kx3z7Pbo-HehPQlN4jOdnPX7k0anA\"\n",
    "client_id = \"Vs4Q7Xf-E_zmCdp_50k08utL9NDFHATRV1NTSQiW7FJPBH2XKnQvMvIyOBJERyav\"\n",
    "access_token = \"HxR7zXc0wRInUmtPm61JS4fpyNnqDUq_Fc8uw_fjfJJ8aKa6TLjHluT08QXNvP8r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:12:33.532123Z",
     "iopub.status.busy": "2022-05-01T11:12:33.531866Z",
     "iopub.status.idle": "2022-05-01T11:12:33.536728Z",
     "shell.execute_reply": "2022-05-01T11:12:33.535757Z",
     "shell.execute_reply.started": "2022-05-01T11:12:33.532094Z"
    },
    "id": "SyXWmVKxdaY5",
    "outputId": "a24fed52-f129-4c00-a35c-c2c6bdc15614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lyricsgenius in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (3.0.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from lyricsgenius) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from lyricsgenius) (2.27.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4>=4.6.0->lyricsgenius) (2.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (1.26.7)\n",
      "Requirement already satisfied: openpyxl in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in /Users/charlieschmitt/opt/anaconda3/lib/python3.7/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lyricsgenius\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:12:47.421255Z",
     "iopub.status.busy": "2022-05-01T11:12:47.420975Z",
     "iopub.status.idle": "2022-05-01T11:12:48.457267Z",
     "shell.execute_reply": "2022-05-01T11:12:48.456521Z",
     "shell.execute_reply.started": "2022-05-01T11:12:47.421223Z"
    },
    "id": "6gANBnORdfHq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import json\n",
    "from lyricsgenius import Genius\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Activation, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.compat.v1.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:13:16.810843Z",
     "iopub.status.busy": "2022-05-01T11:13:16.810238Z",
     "iopub.status.idle": "2022-05-01T11:13:16.817837Z",
     "shell.execute_reply": "2022-05-01T11:13:16.817103Z",
     "shell.execute_reply.started": "2022-05-01T11:13:16.810799Z"
    }
   },
   "outputs": [],
   "source": [
    "# used only for loading rock data\n",
    "\n",
    "def load_rock_data():\n",
    "    df = pd.read_excel(\"./selected_songs.xlsx\")\n",
    "    lyrics = \"\\n\".join(df[\"lyrics\"]).lower().split(\"\\n\")\n",
    "    del df\n",
    "    \n",
    "    return lyrics, False\n",
    "\n",
    "# used only for loading beyonce data\n",
    "\n",
    "def load_beyonce():\n",
    "    if not os.path.exists(\"./Lyrics_Beyoncé.json\"):\n",
    "        genius = Genius(access_token, skip_non_songs=True, \n",
    "                        excluded_terms=[\"(Remix)\", \"(Live)\"],\n",
    "                        remove_section_headers=True, timeout=60)\n",
    "        artist = genius.search_artist('Beyoncé')\n",
    "        artist.save_lyrics()\n",
    "\n",
    "    with open(\"./Lyrics_Beyoncé.json\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        data = json.loads(text)\n",
    "\n",
    "    lyrics = [] \n",
    "    for i in range(len(data[\"songs\"])):\n",
    "        for sentence in data[\"songs\"][i][\"lyrics\"].split(\"\\n\"):\n",
    "            lyrics.append(sentence.lower())\n",
    "            \n",
    "    return lyrics, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:13:50.200025Z",
     "iopub.status.busy": "2022-05-01T11:13:50.199752Z",
     "iopub.status.idle": "2022-05-01T11:13:53.075482Z",
     "shell.execute_reply": "2022-05-01T11:13:53.074537Z",
     "shell.execute_reply.started": "2022-05-01T11:13:50.199994Z"
    },
    "id": "37x59-FBekiZ"
   },
   "outputs": [],
   "source": [
    "# change this line depending on which model you are using\n",
    "lyrics, is_beyonce = load_rock_data()\n",
    "\n",
    "# removing duplicate lines while keeping same order\n",
    "corpus = []\n",
    "text_set = set()\n",
    "\n",
    "for line in lyrics:\n",
    "    if line not in text_set:\n",
    "        corpus.append(line)\n",
    "        text_set.add(line)\n",
    "\n",
    "del text_set\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create sequences of texts. if sequence are length n, then for each element in the sequence\n",
    "# the 0 - (n-1) elements are used to predict the nth element\n",
    "\n",
    "inputs = []\n",
    "for line in corpus:\n",
    "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    for i in range(1, len(tokens)):\n",
    "        seq = tokens[:i+1]\n",
    "\n",
    "        # keep only actual lyrics to beyonce songs (not her speeches, interviews, etc) by restricing length.\n",
    "        # This is not necessary for the rock songs dataframe.\n",
    "        if is_beyonce:\n",
    "            if len(seq) < 25:\n",
    "                inputs.append(seq)\n",
    "        else:\n",
    "            inputs.append(seq)\n",
    "        \n",
    "# pad sequences to all be the length of the longest line\n",
    "max_len = max([len(x) for x in inputs])\n",
    "inputs = np.array(pad_sequences(inputs,\n",
    "                       maxlen = max_len, padding='pre'))\n",
    "\n",
    "# pad sequences to all be the length of the longest line\n",
    "\n",
    "max_len = max([len(x) for x in inputs])\n",
    "inputs = np.array(pad_sequences(inputs,\n",
    "                       maxlen = max_len, padding='pre'))\n",
    "\n",
    "# construct x and y data\n",
    "\n",
    "X_data, y_data = inputs[:, :-1], inputs[:, -1]\n",
    "y_data = to_categorical(y_data, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:16:17.972873Z",
     "iopub.status.busy": "2022-05-01T11:16:17.972592Z",
     "iopub.status.idle": "2022-05-01T11:16:18.604671Z",
     "shell.execute_reply": "2022-05-01T11:16:18.603836Z",
     "shell.execute_reply.started": "2022-05-01T11:16:17.972843Z"
    },
    "id": "dp4pBupoVSaH",
    "outputId": "8bd2c3c0-27b7-48e7-d454-714b042ed15c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 451, 80)           761520    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 451, 160)         103040    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 451, 160)          0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 80)                77120     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               8100      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9519)              961419    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,911,199\n",
      "Trainable params: 1,911,199\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 80, input_length=max_len-1))\n",
    "# Add an LSTM Layer\n",
    "model.add(Bidirectional(LSTM(80, return_sequences=True)))  \n",
    "# A dropout layer for regularisation\n",
    "model.add(Dropout(0.2))\n",
    "# Add another LSTM Layer\n",
    "model.add(LSTM(80))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T11:15:28.027039Z",
     "iopub.status.busy": "2022-05-01T11:15:28.02678Z",
     "iopub.status.idle": "2022-05-01T11:16:10.670711Z",
     "shell.execute_reply": "2022-05-01T11:16:10.669598Z",
     "shell.execute_reply.started": "2022-05-01T11:15:28.02701Z"
    },
    "id": "a0dVabAEXGFV",
    "outputId": "3d73c8c2-b012-4f0b-da60-6aea4dd50ddc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load 150 epoch rock model. Change path name to load other models (i.e. beyonce). \n",
    "# Whenever you change models, make sure to rerun all all previous cells.\n",
    "\n",
    "if not os.path.exists(\"./rock_modelv5.h5\"):\n",
    "    history = model.fit(X_data, y_data, epochs= 150, verbose=1, validation_split=0.2)\n",
    "    model.save(\"./rock_modelv5.h5\")\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"./rock_modelv5.h5\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-28T09:41:19.775013Z",
     "iopub.status.idle": "2022-04-28T09:41:19.775581Z",
     "shell.execute_reply": "2022-04-28T09:41:19.775376Z",
     "shell.execute_reply.started": "2022-04-28T09:41:19.775341Z"
    },
    "id": "u0z9YiS9ejrr",
    "outputId": "8e3532d1-c06c-4c82-ef5c-26155ca21756"
   },
   "outputs": [],
   "source": [
    "def generate(seed_text, num_words):\n",
    "    for i in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list],\n",
    "                    maxlen=max_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0).argmax()\n",
    "        output_word = \"\"\n",
    "\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once i was born to be happy\n",
      "never stop 'til the sun is falling in the clouds\n",
      "once you were the one collecting ideas oooh\n"
     ]
    }
   ],
   "source": [
    "generate(\"once i was\", 4)\n",
    "generate(\"never\", 9)\n",
    "generate(\"once you\", 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
